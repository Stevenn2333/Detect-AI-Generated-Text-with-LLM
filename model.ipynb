{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction from input data using convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional Feature Extraction Model\n",
    "class ConvFeatureExtractionModel(nn.Module):\n",
    "    def __init__(self, input_channels, conv_layers):\n",
    "        super(ConvFeatureExtractionModel, self).__init__()\n",
    "        layers = []\n",
    "        for (out_channels, kernel_size, stride) in conv_layers:\n",
    "            layers.append(nn.Conv1d(input_channels, out_channels, kernel_size=kernel_size, stride=stride))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_channels = out_channels\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cov_Model(nn.Module):\n",
    "    def __init__(self, input_channels, conv_layers, num_tokens, transformer_config):\n",
    "        super(Cov_Model, self).__init__()\n",
    "        self.conv_feature_extractor = ConvFeatureExtractionModel(input_channels, conv_layers)\n",
    "        \n",
    "        # Transformer Encoder Configuration\n",
    "        encoder_layers = nn.TransformerEncoderLayer(**transformer_config)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=transformer_config['num_layers'])\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(transformer_config['d_model'], num_tokens)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Transform input for CNN compatibility: (batch, channels, sequence)\n",
    "        x = x.transpose(1, 2)\n",
    "        conv_features = self.conv_feature_extractor(x)\n",
    "        \n",
    "        # Transpose to fit the transformer input requirements: (sequence, batch, model)\n",
    "        conv_features = conv_features.permute(2, 0, 1)\n",
    "        transformer_output = self.transformer_encoder(conv_features)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(transformer_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class Ro_Model(nn.Module):\n",
    "    def __init__(self, num_classes, transformer_config, roberta_model_name='roberta-base'):\n",
    "        super(Ro_Model, self).__init__()\n",
    "        # Load pre-trained RoBERTa\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        \n",
    "        # Transformer Encoder Configuration\n",
    "        # Creating encoder layer according to transformer_config, excluding 'num_layers'\n",
    "        encoder_layer_config = {k: v for k, v in transformer_config.items() if k != 'num_layers'}\n",
    "        encoder_layers = nn.TransformerEncoderLayer(**encoder_layer_config)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=transformer_config['num_layers'])\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(transformer_config['d_model'], num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the last hidden states from RoBERTa\n",
    "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cls_embeddings = roberta_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(cls_embeddings)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/New_data.csv')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(df['text'].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Prepare labels\n",
    "labels = torch.tensor(df['generated'].values)\n",
    "\n",
    "# Split data into training and validation\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(inputs.input_ids, labels, test_size=0.1)\n",
    "train_masks, val_masks, _, _ = train_test_split(inputs.attention_mask, labels, test_size=0.1)\n",
    "\n",
    "# Create dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create the DataLoader for our training and validation sets\n",
    "train_data = TextDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = TextDataset(val_inputs, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 2  \n",
    "transformer_config = {\n",
    "    'd_model': 768,  # Matches RoBERTa's hidden size\n",
    "    'nhead': 12,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'num_layers': 6,\n",
    "}\n",
    "\n",
    "model = Ro_Model(num_classes, transformer_config)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 Training:  33%|███▎      | 26/78 [1:36:03<3:14:13, 224.11s/it, loss=0.00538]"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 4  # Number of training epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Wrap train_dataloader with tqdm for a progress bar\n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs} Training')\n",
    "    for batch in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        labels = batch['labels'].to(outputs.device)  \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Update the progress bar with the current loss\n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    # Wrap val_dataloader with tqdm for a progress bar\n",
    "    val_progress_bar = tqdm(val_dataloader, desc=f'Epoch {epoch+1}/{epochs} Validation')\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            labels = batch['labels'].to(outputs.device)  \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "            # Update the progress bar with the current validation loss\n",
    "            val_progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "    \n",
    "    print(f\"Validation Loss: {total_eval_loss / len(val_dataloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
